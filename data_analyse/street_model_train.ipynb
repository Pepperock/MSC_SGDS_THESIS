{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import os\n",
    "import keras_preprocessing\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from xgboost import XGBClassifier\n",
    "#from google.colab import files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import process_tool\n",
    "import model1\n",
    "import coatnet\n",
    "import DLmodels\n",
    "import preprocess \n",
    "from process_tool import empty_col\n",
    "from process_tool import name_to_num\n",
    "from process_tool import num_to_name\n",
    "from process_tool import array_valid\n",
    "from process_tool import read_img_sa\n",
    "from process_tool import read_img_street\n",
    "from process_tool import create_street_tb\n",
    "from model1 import dense_tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "safe_street = pd.read_pickle('./safe_data/safe_street.pkl')\n",
    "safe_street = preprocess.dense_preprocess(safe_street, 'street', 'dense')\n",
    "safe_street = preprocess.vgg16_preprocess(safe_street, 'street', 'vgg16')\n",
    "safe_street = preprocess.eff_preprocess(safe_street, 'street', 'efficent')\n",
    "safe_street = preprocess.res_preprocess(safe_street, 'street', 'resnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eff_x = safe2015['efficent']\n",
    "eff_x = array_valid(eff_x)\n",
    "feature_eff = DLmodels.eff_tb(eff_x)\n",
    "\n",
    "vgg_x = safe2015['vgg16']\n",
    "vgg_x = array_valid(vgg_x)\n",
    "feature_vgg16 = DLmodels.vgg16_tb(vgg_x)\n",
    "\n",
    "coat_x = safe2015['street']\n",
    "coat_x = array_valid(coat_x)\n",
    "feature_coat= DLmodels.coat_tb(coat_x)\n",
    "\n",
    "res_x = safe2015['resnet']\n",
    "res_x = array_valid(res_x)\n",
    "feature_res = DLmodels.res_tb(res_x)\n",
    "\n",
    "den_x = safe2015['dense']\n",
    "den_x = array_valid(den_x)\n",
    "feature_dense = DLmodels.dense_tb(den_x)\n",
    "\n",
    "#feature_res.to_csv('./safe_data/res_feature_safe.csv')\n",
    "#feature_eff.to_csv('./safe_data/eff_feature_safe.csv')\n",
    "#feature_vgg16.to_csv('./safe_data/vgg_feature_safe.csv')\n",
    "#feature_coat.to_csv('./safe_data/coat_feature_safe.csv')\n",
    "#feature_dense.to_csv('./data_collection_aug/street_features/safe_street_feature.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test logistic regression for vgg, effinet, coatnet, resnet, and densenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "X = feature_res\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "y = safe2015['class']\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, random_state = 0, test_size = 0.25)\n",
    "\n",
    "lgr = LogisticRegression(max_iter = 2000)\n",
    "lgr.fit(x_train, y_train)\n",
    "pred_lgr = lgr.predict(x_test)\n",
    "acc_lgr_test = accuracy_score(pred_lgr, y_test)\n",
    "f1_lgr_test = f1_score(pred_lgr, y_test)\n",
    "pred_train = lgr.predict(x_train)\n",
    "acc_lgr_train = accuracy_score(pred_train, y_train)\n",
    "f1_lgr_train = f1_score(pred_train, y_train)\n",
    "print(acc_lgr_train, acc_lgr_test, f1_lgr_train, f1_lgr_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = feature_coat\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "y = safe2015['class']\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, random_state = 0, test_size = 0.25)\n",
    "\n",
    "lgr = LogisticRegression(max_iter = 2000)\n",
    "lgr.fit(x_train, y_train)\n",
    "pred_lgr = lgr.predict(x_test)\n",
    "acc_lgr_test = accuracy_score(pred_lgr, y_test)\n",
    "f1_lgr_test = f1_score(pred_lgr, y_test)\n",
    "pred_train = lgr.predict(x_train)\n",
    "acc_lgr_train = accuracy_score(pred_train, y_train)\n",
    "f1_lgr_train = f1_score(pred_train, y_train)\n",
    "print(acc_lgr_train, acc_lgr_test, f1_lgr_train, f1_lgr_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = feature_vgg16\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "y = safe2015['class']\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, random_state = 0, test_size = 0.25)\n",
    "\n",
    "lgr = LogisticRegression(max_iter = 2000)\n",
    "lgr.fit(x_train, y_train)\n",
    "pred_lgr = lgr.predict(x_test)\n",
    "acc_lgr_test = accuracy_score(pred_lgr, y_test)\n",
    "f1_lgr_test = f1_score(pred_lgr, y_test)\n",
    "pred_train = lgr.predict(x_train)\n",
    "acc_lgr_train = accuracy_score(pred_train, y_train)\n",
    "f1_lgr_train = f1_score(pred_train, y_train)\n",
    "print(acc_lgr_train, acc_lgr_test, f1_lgr_train, f1_lgr_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = feature_dense\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "y = safe2015['class']\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, random_state = 0, test_size = 0.25)\n",
    "\n",
    "lgr = LogisticRegression(max_iter = 2000)\n",
    "lgr.fit(x_train, y_train)\n",
    "pred_lgr = lgr.predict(x_test)\n",
    "acc_lgr_test = accuracy_score(pred_lgr, y_test)\n",
    "f1_lgr_test = f1_score(pred_lgr, y_test)\n",
    "pred_train = lgr.predict(x_train)\n",
    "acc_lgr_train = accuracy_score(pred_train, y_train)\n",
    "f1_lgr_train = f1_score(pred_train, y_train)\n",
    "print(acc_lgr_train, acc_lgr_test, f1_lgr_train, f1_lgr_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVC for all model of safety perception "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "X = feature_res\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "y = safe_street['class']\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, random_state = 0, test_size = 0.25)\n",
    "\n",
    "svm = SVC()\n",
    "svm.fit(x_train, y_train)\n",
    "pred_svm = svm.predict(x_test)\n",
    "pred_svm2 = svm.predict(x_train)\n",
    "acc_svm = accuracy_score(pred_svm, y_test)\n",
    "acc_svm2 = accuracy_score(pred_svm2, y_train)\n",
    "print(accuracy_score(pred_svm2, y_train), acc_svm, f1_score(pred_svm2, y_train), f1_score(pred_svm, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = feature_vgg16\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "y = safe_street['class']\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, random_state = 0, test_size = 0.25)\n",
    "\n",
    "svm = SVC()\n",
    "svm.fit(x_train, y_train)\n",
    "pred_svm = svm.predict(x_test)\n",
    "pred_svm2 = svm.predict(x_train)\n",
    "acc_svm = accuracy_score(pred_svm, y_test)\n",
    "acc_svm2 = accuracy_score(pred_svm2, y_train)\n",
    "print(accuracy_score(pred_svm2, y_train), acc_svm, f1_score(pred_svm2, y_train), f1_score(pred_svm, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = feature_coat\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "y = safe_street['class']\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, random_state = 0, test_size = 0.25)\n",
    "\n",
    "svm = SVC()\n",
    "svm.fit(x_train, y_train)\n",
    "pred_svm = svm.predict(x_test)\n",
    "pred_svm2 = svm.predict(x_train)\n",
    "acc_svm = accuracy_score(pred_svm, y_test)\n",
    "acc_svm2 = accuracy_score(pred_svm2, y_train)\n",
    "print(accuracy_score(pred_svm2, y_train), acc_svm, f1_score(pred_svm2, y_train), f1_score(pred_svm, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = feature_eff\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "y = safe_street['class']\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, random_state = 0, test_size = 0.25)\n",
    "\n",
    "svm = SVC()\n",
    "svm.fit(x_train, y_train)\n",
    "pred_svm = svm.predict(x_test)\n",
    "pred_svm2 = svm.predict(x_train)\n",
    "acc_svm = accuracy_score(pred_svm, y_test)\n",
    "acc_svm2 = accuracy_score(pred_svm2, y_train)\n",
    "print(accuracy_score(pred_svm2, y_train), acc_svm, f1_score(pred_svm2, y_train), f1_score(pred_svm, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = feature_dense\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "y = safe_street['class']\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, random_state = 0, test_size = 0.25)\n",
    "\n",
    "svm = SVC()\n",
    "svm.fit(x_train, y_train)\n",
    "pred_svm = svm.predict(x_test)\n",
    "pred_svm2 = svm.predict(x_train)\n",
    "acc_svm = accuracy_score(pred_svm, y_test)\n",
    "acc_svm2 = accuracy_score(pred_svm2, y_train)\n",
    "print(accuracy_score(pred_svm2, y_train), acc_svm, f1_score(pred_svm2, y_train), f1_score(pred_svm, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Res vs Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wealth_street = pd.read_pickle('./data/wealth_street.pkl')\n",
    "wealth_street = preprocess.dense_preprocess(wealth_street, 'street', 'dense')\n",
    "wealth_street = preprocess.vgg16_preprocess(wealth_street, 'street', 'vgg16')\n",
    "lively_street = pd.read_pickle('./lively_data/lively_street.pkl')\n",
    "lively_street = preprocess.dense_preprocess(lively_street, 'street', 'dense')\n",
    "lively_street = preprocess.vgg16_preprocess(lively_street, 'street', 'vgg16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wealth_den = wealth_street['dense']\n",
    "wealth_den = array_valid(wealth_den)\n",
    "wealth_den_feature = DLmodels.dense_tb(wealth_den)\n",
    "lively_den = lively_street['dense']\n",
    "lively_den = array_valid(lively_den)\n",
    "lively_den_feature = DLmodels.dense_tb(lively_den)\n",
    "\n",
    "wealth_res = wealth_street['resnet']\n",
    "wealth_res = array_valid(wealth_res)\n",
    "wealth_res_feature = DLmodels.res_tb(wealth_res)\n",
    "lively_res = lively_street['resnet']\n",
    "lively_res = array_valid(lively_res)\n",
    "lively_res_feature = DLmodels.res_tb(lively_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = wealth_res_feature\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "y = safe_street['class']\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, random_state = 0, test_size = 0.25)\n",
    "\n",
    "svm = SVC()\n",
    "svm.fit(x_train, y_train)\n",
    "pred_svm = svm.predict(x_test)\n",
    "pred_svm2 = svm.predict(x_train)\n",
    "acc_svm = accuracy_score(pred_svm, y_test)\n",
    "acc_svm2 = accuracy_score(pred_svm2, y_train)\n",
    "print(accuracy_score(pred_svm2, y_train), acc_svm, f1_score(pred_svm2, y_train), f1_score(pred_svm, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = wealth_den_feature\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "y = safe_street['class']\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, random_state = 0, test_size = 0.25)\n",
    "\n",
    "svm = SVC()\n",
    "svm.fit(x_train, y_train)\n",
    "pred_svm = svm.predict(x_test)\n",
    "pred_svm2 = svm.predict(x_train)\n",
    "acc_svm = accuracy_score(pred_svm, y_test)\n",
    "acc_svm2 = accuracy_score(pred_svm2, y_train)\n",
    "print(accuracy_score(pred_svm2, y_train), acc_svm, f1_score(pred_svm2, y_train), f1_score(pred_svm, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = lively_res_feature\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "y = safe_street['class']\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, random_state = 0, test_size = 0.25)\n",
    "\n",
    "svm = SVC()\n",
    "svm.fit(x_train, y_train)\n",
    "pred_svm = svm.predict(x_test)\n",
    "pred_svm2 = svm.predict(x_train)\n",
    "acc_svm = accuracy_score(pred_svm, y_test)\n",
    "acc_svm2 = accuracy_score(pred_svm2, y_train)\n",
    "print(accuracy_score(pred_svm2, y_train), acc_svm, f1_score(pred_svm2, y_train), f1_score(pred_svm, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = lively_den_feature\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "y = safe_street['class']\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, random_state = 0, test_size = 0.25)\n",
    "\n",
    "svm = SVC()\n",
    "svm.fit(x_train, y_train)\n",
    "pred_svm = svm.predict(x_test)\n",
    "pred_svm2 = svm.predict(x_train)\n",
    "acc_svm = accuracy_score(pred_svm, y_test)\n",
    "acc_svm2 = accuracy_score(pred_svm2, y_train)\n",
    "print(accuracy_score(pred_svm2, y_train), acc_svm, f1_score(pred_svm2, y_train), f1_score(pred_svm, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tuning of SVM with densenet features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = feature_dense\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "y = safe_street['class']\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, random_state = 0, test_size = 0.25)\n",
    "\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV \n",
    "from time import time\n",
    "clf = SVC()\n",
    "param_grid = {'C': [1, 10, 100], 'gamma':[0.0001,0.001,0.1,1],'kernel': ['linear']}\n",
    "grid_search = GridSearchCV(clf, param_grid=param_grid)\n",
    "start = time()\n",
    "grid_search.fit(x_train, y_train)\n",
    "\n",
    "def report(results, n_top=3):\n",
    "    for i in range(1, n_top + 1):\n",
    "        candidates = numpy.flatnonzero(results[\"rank_test_score\"] == i)\n",
    "        for candidate in candidates:\n",
    "            print(\"Model with rank: {0}\".format(i))\n",
    "            print(\n",
    "                \"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
    "                    results[\"mean_test_score\"][candidate],\n",
    "                    results[\"std_test_score\"][candidate],\n",
    "                )\n",
    "            )\n",
    "            print(\"Parameters: {0}\".format(results[\"params\"][candidate]))\n",
    "            print(\"\")\n",
    "\n",
    "print(\n",
    "    \"GridSearchCV took %.2f seconds for %d candidate parameter settings.\"\n",
    "    % (time() - start, len(grid_search.cv_results_[\"params\"]))\n",
    ")\n",
    "report(grid_search.cv_results_)\n",
    "# after iteratively testing for the params, we found best gamma of 0.0005 which is very close to default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params1 = {\n",
    "    'n_estimators': [100, 500, 1000],\n",
    "    'learning_rate': [0.01, 0.1],\n",
    "}\n",
    "\n",
    "params2 = {\n",
    "    'max_depth': [10, 15, 20],\n",
    "    'min_chid_weight': [1,5,8]\n",
    "}\n",
    "\n",
    "other_params = {'learning_rate': 0.01, 'n_estimators': 500, 'max_depth': 15, 'min_child_weight': 1, 'seed': 0,\n",
    "                    'subsample': 0.8, 'colsample_bytree': 0.8, 'gamma': 0, 'reg_alpha': 0, 'reg_lambda': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = XGBClassifier(**other_params)\n",
    "optimized_GBM = GridSearchCV(estimator=model, param_grid= params1, scoring='accuracy', cv=4, verbose=1)\n",
    "optimized_GBM.fit(x_train, y_train)\n",
    "print('gsearch1.best_score_', optimized_GBM.best_score_)\n",
    "print('gsearch1.best_param_', optimized_GBM.best_params_)\n",
    "# learning rate 0.1, n = 500, max_depth = 15 for best performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(max_depth=15, random_state=0, n_estimators= 500)\n",
    "clf.fit(x_train, y_train)\n",
    "pred_rf = clf.predict(x_test)\n",
    "pred_rf2 = clf.predict(x_train)\n",
    "acc_rf = accuracy_score(pred_rf, y_test)\n",
    "acc_rf2 = accuracy_score(pred_rf2, y_train)\n",
    "print(acc_rf2, acc_rf, f1_score(pred_rf2, y_train), f1_score(pred_rf, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fit and export the models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## xgb models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBClassifier(n_estimators = 500, max_depth = 15, learning_rate = 0.1)\n",
    "svm = SVC()\n",
    "\n",
    "X = wealth_den_feature\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "y = wealth_street['class']\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, random_state = 0, test_size = 0.25)\n",
    "wealth_xgb = xgb.fit(x_train, y_train, probability = True)\n",
    "wealth_svm = svm.fit(x_train, y_train, probability = True)\n",
    "\n",
    "file1 = \"./models/svm_wealth_model.sav\"\n",
    "file2 = \"./models/xgb_wealth_model.sav\"\n",
    "pickle.dump(wealth_svm, open(file1, 'wb'))\n",
    "pickle.dump(wealth_xgb, open(file1, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "safe_den_feature =  feature_dense\n",
    "\n",
    "X = safe_den_feature\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "y = safe_street['class']\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, random_state = 0, test_size = 0.25)\n",
    "safe_xgb = xgb.fit(x_train, y_train, probability = True)\n",
    "safe_svm = svm.fit(x_train, y_train, probability = True)\n",
    "\n",
    "file1 = \"./models/svm_safe_model.sav\"\n",
    "file2 = \"./models/xgb_safe_model.sav\"\n",
    "pickle.dump(safe_svm, open(file1, 'wb'))\n",
    "pickle.dump(safe_xgb, open(file1, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = lively_den_feature\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "y = lively_street['class']\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, random_state = 0, test_size = 0.25)\n",
    "lively_xgb = xgb.fit(x_train, y_train, probability = True)\n",
    "lively_svm = svm.fit(x_train, y_train, probability = True)\n",
    "\n",
    "file1 = \"./models/svm_lively_model.sav\"\n",
    "file2 = \"./models/xgb_lively_model.sav\"\n",
    "pickle.dump(lively_svm, open(file1, 'wb'))\n",
    "pickle.dump(lively_xgb, open(file1, 'wb'))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
